ssignment - Local Hadoop Streaming and AWS
 
All the tasks should be done using Hadoop Streaming (Hadoop version 2.x) and Python.

You will execute all the tasks TWICE:

First, you will run them locally (Local Hadoop or Cloudera VM) to test whether the logic of your job is correct. You may also use the testing tool to help verify the correctness of your solution.

Then, you will run them on AWS over a bigger data set (using the command line interface).

You need to submit 3 files to NYU classes: {netid}_sources.zip, {netid}_all.zip, aws.txt. See below for details.

 

1) Local Hadoop (submit {netid}_sources.zip and {netid}_all.zip)
- You will use the following input data:
    Taxi Data for two days: https://www.mediafire.com/folder/3m9a39g8n0jyd/Taxi_Data_-_Assignment_3
    Vehicle Data: http://www.mediafire.com/view/6wziuzg5983q9oq/licenses.csv

- Use two reducers, unless noted otherwise

- Submit two .zip files, one named {netid}_sources.zip, the other named {netid}_all.zip (replace {netid} by your netId). Both zip files contain the following directory structure:
    * One directory per task, named "taskX", where X is the number of the task.
    * If a task has a sub-task, use "taskX-Y", where Y identifies the sub-task.
      E.g.: "task2-a" refers to sub-task "a" of Task 2. If you do an “ls” on your homework directory, it should contain the following sub-directories

task1
task2-a
task2-b
task2-c
task2-d
task2-e
task2-f
task3
task4-a
task4-b

Inside each subdirectory, include:

* The Python scripts used by your map-reduce job. Name your map script as map.py, and your reduce script as reduce.py

* In {netid}_all.zip, also include the output directory generated by Hadoop (use the naming convention specified below). DO NOT include the outputs in {netid}_sources.zip.

* DO NOT submit the input data

 

*** Your program shall work correctly with the default partitioner and combiner. Each mapper will receive a portion of one of the input file, with lines coming in arbitrary order. Each reducer will receive a subset of keys with their values of the mapper output, with the keys sorted, but the values coming in arbitrary order.

    Example: If an input file contains 5 lines: {a, b, c, d, e}. It is possible that one of your mapper receives {a, c, d}, while another mapper receives {b, e}.

    Example: If your mappers together output 4 lines: {a, b, c, d}. It is possible that one of your reducer receives {a, c}, while another reducer receives {b, d}.

    Note: Some tasks specially enforce 1 reducer, which will be explicitly specified. Otherwise, your codes shall work with multiple mappers/reducers.

*** Your map.py codes are permitted to access the mapreduce_map_input_file environment variable in order to determine which input (.csv) file is being read. The CSV filenames will contain the substrings "trip", "fare", and "license". You may not use any other environment variables besides mapreduce_map_input_file.

    Example: If the sample table name is "fare_data.csv", the code

os.environ.get("mapreduce_map_input_file")
      will be able to retrieve this string so that you know which file this line is coming from.

    Note: If a sample input file name is something like "part-00000", then the task only has a unique input table and you do not need to use the environment variable.

*** Local Hadoop Testing Tool

The testing tool is available at http://cs.nyu.edu/~by460/task-runner/

Your code must at least pass the testing tool in order to receive credits for a task. Passing the testing tool does not guarantee passing the final tests, as in the SQL-assignment. Only submit {netid}_sources.zip to the testing tool.

The delimiter of task-runner between key and value is '\t'.

 

2) AWS (submit aws.txt)
- You will use the following input data:
    Taxi Data for one week: https://www.mediafire.com/folder/1zaqzcf722loc/taxi_data_aug_week1
    Vehicle Data: http://www.mediafire.com/view/6wziuzg5983q9oq/licenses.csv

- Use two reducers, unless noted otherwise
- Your output files should reside in an S3 PUBLIC bucket (see https://aws.amazon.com/articles/5050).  You should include links to all output files in a text file named aws.txt with the following structure:
    
task1
link 1
link 2

task2-a
link 1
...

- DO NOT submit links to the input data
- DO NOT submit links to your Python scripts -- they should be the same as the ones used for the Local Hadoop execution

 

===================================================================
Task 1
  - Write a map-reduce job that joins the 'trips' and 'fare' data (taxi data).
  - Note that the 'fares' and 'trips' data share 4 attributes: medallion, hack_license, vendor_id, pickup_datetime.
  - The join MUST BE a reduce-side inner join.
  - Output: A key-value pair per line — use a “tab” to separate the key and the value, a comma between  — where
    
    key: medallion, hack_license, vendor_id, pickup_datetime
    value: remaining attributes of 'trips' data in their original order, and  the remaining attributes of 'fare' data in their original order

    You must respect this ordering!

Here’s a sample output with 2 key-value pairs:
00005007A9F30E289E760362F69E4EAD,2C584442C9DC6740767CDE5672C12379,CMT,2013-08-07 00:55:11    1,N,2013-08-07 00:25:38,1,990,8.9,-73.981972,40.764397,-73.927887,40.865353,CRD,26.5,0.5,0.5,5.5,0,33
00005007A9F30E289E760362F69E4EAD,2C584442C9DC6740767CDE5672C12379,CMT,2013-08-07 02:01:47    1,N,2013-08-07 01:06:05,1,653,3.3,-73.983887,40.780346,-73.991646,40.744511,CSH,12.5,0.5,0.5,0,0,13.5


  - The output directory produced by Hadoop should be named TripFareJoin. Here’s what the contents of the task1 subdirectory should look like:

ls -F task1/
TripFareJoin/    map.py*        reduce.py*

Note: The output directory "TripFareJoin" is not to be included in {netid}_sources.zip. Only include "TripFareJoin" in {netid}_all.zip.

===================================================================
Task 2

Note: Similar to Task 1, you must use a tab to separate the key and the value in the output tuples. 

Write map-reduce jobs for each of the following sub-tasks, using the output of Task 1 (joined data) as input for all these sub-tasks:

    a) Find the distribution of fare amounts (fare_amount) for each of the the following ranges: [0,4], [4.01,8], [8.01,12], [12.01, 16], [16.01, 20], [20.01, 24], [24.01, 28], [28.01, 32], [32.01, 36], [36.01, 40], [40.01, 44], [44.01, 48], [48.01, infinite], i.e., for each range, the number of trips whose fare amount falls in the range.
       Output: A key-value pair per line, where the key is the range, and the value is the number of trips. For example,
       0,4    100
    4.01,8    300
    …
       The output directory produced by Hadoop should be named FareAmounts.
    b) Find the number of trips that cost less than or equal than $10 (total_amount).
       Output: The number of trips.
       The output directory produced by Hadoop should be named TripAmount.

       Note: This task enforces 1 reducer.

    c) Find the distribution of the number of passengers, i.e., for each number of passengers A, the number of trips that had A passengers.
       Output: A key-value pair per line, where the key is the number of passengers, and the value is the number of trips.
       The output directory produced by Hadoop should be named NumberPassengers.
    d) Find the total revenue (for all taxis) and the total amount spent on tolls per day (from pickup_datetime). The revenue should include the fare amount, tips, and surcharges.
       Output: A key-value pair per line, where the key is the day (YYYY-MM-DD), and the value contains the total revenue and the total tolls for that day, in this order.
       The values in the output must have a precision of two decimal digits, e.g., 3.02245 should be represented as 3.02.
    For example,
       2016-01-01    100000.02,11000.00
       2016-01-02    202000.00,1000.00

       The output directory produced by Hadoop should be named TotalRevenue.
    e) For each taxi (medallion) find the total number of trips and the average number of trips per day. For the average trips per day, use 2 decimal digits. 
       Output: A key-value pair per line, where the key is the medallion, and the value contains the total number of trips and the average number of trips per day.
       The output directory produced by Hadoop should be named MedallionTrips.
    f) Find the number of different taxis (medallion) used by each driver (license).
       Output: A key-value pair per line, where the key is the driver, and the value is the number of different taxis used by that driver.
       The output directory produced by Hadoop should be named UniqueTaxis.

===================================================================

Task 3
  - Write a map-reduce job that joins the output from Task 1 with the vehicle data.
  - Note that they both share the medallion attribute.
  - The join MUST BE a reduce-side inner join.
  - Output: A key-value pair per line, where
    
    key: medallion
    value: remaining attributes of Task 1 output (including the remaining keys) in their original order + remaining attributes of vehicle data in their original order

    You should respect this ordering!

  - The output directory produced by Hadoop should be named VehicleJoin.

===================================================================

Task 4
Create map-reduce jobs for the following sub-tasks, using the output from Task 3 as input.
    
Note: In the vehicle data, you may find attributes with commas in the value, so splitting the line by the comma character may not work when reading the attributes (you may end up splitting one attribute in two or more). You can use the csv module (https://docs.python.org/2/library/csv.html) to parse each line; since this module assumes a file as input (not a string), you will need to use StringIO (https://docs.python.org/2/library/stringio.html) as well. Here’s an example:

    csv_file = StringIO.StringIO(line)
    csv_reader = csv.reader(csv_file)
    for record in csv_reader:
        # record is a list containing all the attributes
    a) Compare trips based on vehicle_type (WAV, HYB, CNG, LV1, DSE, NRML).
       Output: A key-value pair per line, where the key is the vehicle type, and the value contains the total number of trips, the total revenue, and the average tip percentage (based on the total revenue), in this order.
       All the non-integer values in the output must have a precision of two decimal digits.
       The output directory produced by Hadoop should be named VehicleType.
       Note: if total revenue is zero, then tip percentage is zero as well.
    b) List the top 20 agents by total revenue.
       Output: A key-value pair per line, where the key is the agent name, and the value contains the total revenue.
       All the values in the output must have a precision of two decimal digits.
       The output directory produced by Hadoop should be named Top20Revenue.

       Note: This task enforces 1 reducer. (otherwise you may have one Top K for each reducer)